{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RERobbins/data_science_266_sandbox/blob/main/3_QA_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An Introduction to Question Answering Using Retrieval Augmented Generation\n",
        "\n",
        "In our introduction to vector databases, we noted that retrieval augment generation or \"RAG\" is used in tasks like question answering. With RAG, a retrieval component first selects a set of relevant documents or passages from a larger corpus, and then a generation component generates the final response based on the selected information. This approach aims to combine the accuracy of retrieval with the flexibility of generation.\n",
        "\n",
        "This notebook builds on our work with vector databases.  We will take a query from the user and present it to the vector database. Then, we present that query and the relevant documents to the generative model to generate an answer.\n",
        "\n",
        "We will make use of LangChain to coordinate this activity and explore model prompting.\n",
        "\n",
        "We will answer questions about the privacy policies we have been working with.\n",
        "\n",
        "# Generative Models, Embedding Models and Vector Databases\n",
        "\n",
        "This notebook assumes that you use the same generative model throughout.  You will rely on the API keys you needed to work on the Embeddings notebook.\n",
        "\n",
        "This notebook uses the same SentenceTransformer embedding model we used in the Vector Database notebook.  As was the case with that notebook, you are free to select another embedding model.\n",
        "\n",
        "Finally, this notebook will use the Qdrant vector database.  You are, of course, free to experiment with other vector databases as we discussed before."
      ],
      "metadata": {
        "id": "Pibz6CKofUoT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhQ4p1pSkFfc"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVFr3Prq8ngq"
      },
      "source": [
        "## Environment Related Helpers\n",
        "\n",
        "This portion of the notebook includes `install_if_needed` which will install a single package or list of packages with `pip` only if necessary, and `running_in_colab` a predicate that returns `True` if the notebook is running in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "M7cvKLyL9FxG"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "\n",
        "\n",
        "def install_if_needed(package_names):\n",
        "    \"\"\"\n",
        "    Install one or more Python packages using pip if they are not already installed.\n",
        "\n",
        "    Args:\n",
        "        package_names (str or list): The name(s) of the package(s) to install.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if isinstance(package_names, str):\n",
        "        package_names = [package_names]\n",
        "\n",
        "    for package_name in package_names:\n",
        "        try:\n",
        "            importlib.import_module(package_name)\n",
        "            print(f\"{package_name} is already installed.\")\n",
        "        except ImportError:\n",
        "            !pip install --quiet {package_name}\n",
        "            print(f\"{package_name} has been installed.\")\n",
        "\n",
        "\n",
        "def running_in_colab():\n",
        "    \"\"\"\n",
        "    Check if the Jupyter Notebook is running in Google Colab.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if running in Google Colab, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teyGc1Hox8Hb"
      },
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "By default, the data you create in Google Colaboratory does not persist from session to session.  Each session runs in a virtual machine and when that machine goes away, so does your data.  If you want your data to persist, you must store it outside the virtual machine. Google Drive can be used for that purpose.  We use it later in this notebook to store the OpenAI and Cohere API keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UWg8VEQ55bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ec5892-3900-4f16-f20c-58ebd6fe23b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if running_in_colab():\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkumrUT0qR4F"
      },
      "source": [
        "## API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yoDbPoXiQiQ",
        "outputId": "83919fd5-349f-43f9-9fcb-f33f3c160f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python-dotenv has been installed.\n"
          ]
        }
      ],
      "source": [
        "install_if_needed(\"python-dotenv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "def env_file_path(\n",
        "    colab_path=\"/content/drive/MyDrive/.env\", other_path=f\"{find_dotenv()}\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns the appropriate file path for the environment variables file (.env) based on the execution environment.\n",
        "\n",
        "    This function is designed to determine the correct path for the environment variables file\n",
        "    depending on whether the code is running in Google Colab or in a different environment.\n",
        "\n",
        "    Args:\n",
        "        colab_path (str, optional): The file path for the environment variables file in Google Colab.\n",
        "            Default is '/content/drive/MyDrive/.env'.\n",
        "\n",
        "        other_path (str, optional): The file path for the environment variables file in other environments.\n",
        "            Default is '/workspace/.env'.\n",
        "\n",
        "    Returns:\n",
        "        str: The file path for the environment variables file (.env).\n",
        "    \"\"\"\n",
        "\n",
        "    return colab_path if running_in_colab() else other_path"
      ],
      "metadata": {
        "id": "xW35m2pUl6bz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RqQGWpwx8u4A"
      },
      "outputs": [],
      "source": [
        "load_dotenv(env_file_path())\n",
        "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "COHERE_API_KEY = os.environ[\"COHERE_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKu1ACMquIIF"
      },
      "source": [
        "## Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_qMdGLwvG_6",
        "outputId": "27f8fcf2-36a7-4343-e515-461233be6c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain is already installed.\n"
          ]
        }
      ],
      "source": [
        "install_if_needed(\"langchain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DXbWTaH7EJ4"
      },
      "source": [
        "## GPU Support (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT2tdLfx7JdH",
        "outputId": "ab3e840e-e031-478e-da8b-fa61c0d56010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices(\"GPU\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8L8VHnu8qko",
        "outputId": "d2f3240d-bdd5-4f9a-8b99-2a49324cf24b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch is already installed.\n",
            "CUDA Available: True\n"
          ]
        }
      ],
      "source": [
        "install_if_needed(\"torch\")\n",
        "import torch\n",
        "\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVNMF4FR-Up3"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMf9o9-V3tP6"
      },
      "source": [
        "Instantiate the embeddings model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GTI7DbH-4E-",
        "outputId": "3a1522c1-eb08-4b03-f31f-23b9fa3be4d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openai is already installed.\n",
            "cohere is already installed.\n",
            "tiktoken is already installed.\n",
            "transformers is already installed.\n",
            "sentence_transformers is already installed.\n"
          ]
        }
      ],
      "source": [
        "packages = [\n",
        "    \"openai\",\n",
        "    \"cohere\",\n",
        "    \"tiktoken\",\n",
        "    \"transformers\",\n",
        "    \"sentence_transformers\",\n",
        "]\n",
        "\n",
        "install_if_needed(packages)\n",
        "\n",
        "import openai, tiktoken\n",
        "\n",
        "import cohere\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import Cohere\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XqqeQXVyFE8n"
      },
      "outputs": [],
      "source": [
        "st_model_name = \"multi-qa-mpnet-base-cos-v1\"\n",
        "st_embeddings_model = HuggingFaceEmbeddings(model_name=st_model_name)\n",
        "st_tokenizer = AutoTokenizer.from_pretrained(f\"sentence-transformers/{st_model_name}\")\n",
        "\n",
        "embeddings_model = st_embeddings_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-zF6E82OES8"
      },
      "source": [
        "## Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McW_mfokeGZy",
        "outputId": "04ec28b6-81d6-4970-f154-7796b72694ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qdrant-client has been installed.\n"
          ]
        }
      ],
      "source": [
        "install_if_needed(\"qdrant-client\")\n",
        "from langchain.vectorstores import Qdrant\n",
        "\n",
        "import qdrant_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDPR34aEOEe5"
      },
      "source": [
        "# Generative Model\n",
        "\n",
        "OpenAI trial accounts expire after three months and provide access to `gpt-3.5-turbo` but not `gpt-4`.  Paid OpenAI accounts permit use of `gpt-4` as well and do not expire.  Cohere trial accounts do not expire, but the API rate limiting is more significant than OpenAI trial account rate limiting.\n",
        "\n",
        "Set the `LLM` variable below to reflect the generative model you want to use.  \n",
        "\n",
        "The results in most of the examples below will vary with your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "BwggMZsAamu8"
      },
      "outputs": [],
      "source": [
        "# llm = Cohere(model=\"command\", temperature=0)\n",
        "LLM = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "# llm = ChatOpenAI(model=\"gpt-4\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afB_K7CtfKUF"
      },
      "source": [
        "# Load Documents, Split Into Chunks, Create Vector Database\n",
        "\n",
        "**If you saved you Qdrant database when you worked on the vector database notebook you can skip this section.**\n",
        "\n",
        "Otherwise, we use the following cells to download the privacy policies we have been working with and split them into chunks to be stored in the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwfjt4R5hMy6",
        "outputId": "a9cf715c-1e00-4152-f980-66da078d9751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pypdf is already installed.\n",
            "unstructured is already installed.\n"
          ]
        }
      ],
      "source": [
        "install_if_needed([\"pypdf\", \"unstructured\"])\n",
        "\n",
        "import textwrap\n",
        "from langchain.document_loaders import PyPDFLoader, UnstructuredURLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Ch1pJycHjh0L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "policy_data = [\n",
        "    (\"Apple\",\n",
        "     \"Privacy Policy\",\n",
        "     \"https://www.apple.com/legal/privacy/pdfs/apple-privacy-policy-en-ww.pdf\",\n",
        "    ),\n",
        "    (\"Cohere\", \"Privacy Policy\", \"https://cohere.com/privacy\"),\n",
        "    (\"Google\",\n",
        "     \"Privacy Policy\",\n",
        "     \"https://static.googleusercontent.com/media/www.google.com/en//intl/en/policies/privacy/google_privacy_policy_en.pdf\",\n",
        "    ),\n",
        "    (\"Hugging Face\", \"Privacy Policy\", \"https://huggingface.co/privacy\"),\n",
        "    (\"Meta\",\n",
        "     \"Privacy Policy\",\n",
        "     \"https://about.fb.com/wp-content/uploads/2022/07/Privacy-Within-Metas-Integrity-Systems.pdf\",\n",
        "    ),\n",
        "    (\"Threads\", \"Privacy Policy\", \"https://terms.threads.com/privacy-policy\"),\n",
        "    (\"TikTok\",\n",
        "     \"Privacy Policy\",\n",
        "     \"https://www.tiktok.com/legal/page/us/privacy-policy/en\",\n",
        "    ),]\n",
        "\n",
        "columns = [\"organization\", \"title\", \"url\"]\n",
        "\n",
        "policy_df = pd.DataFrame(policy_data, columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aVsFWsVbAVFm"
      },
      "outputs": [],
      "source": [
        "def get_chunks(url, organization, title, chunk_size=2000, chunk_overlap=500):\n",
        "    \"\"\"\n",
        "    This function takes a url to an organization's web page, organization name,\n",
        "    and document title and returns chunks constructed from the target url.\n",
        "    The function adds the url, the organization name and the document title\n",
        "    as metadata to the chunks.\n",
        "\n",
        "    Parameters:\n",
        "    url (string): Target page.\n",
        "    organization (string): Organization name.\n",
        "    title: Document title.\n",
        "    chunk_size (int, optional): Chunk size, default is 2000 characters.\n",
        "    chunk_overlap (int, optional): Chunk overlap, default is 500 characters.\n",
        "\n",
        "    Returns:\n",
        "    list of chunks\n",
        "    \"\"\"\n",
        "\n",
        "    # Use PyPDFLoader for pdf targets, otherwise UnstructuredURLLoader\n",
        "    if os.path.splitext(url)[1] == \".pdf\":\n",
        "        loader = PyPDFLoader(url)\n",
        "    else:\n",
        "      loader = UnstructuredURLLoader([url])\n",
        "\n",
        "    documents = loader.load()\n",
        "    for document in documents:\n",
        "        metadata = document.metadata\n",
        "        metadata[\"url\"] = url\n",
        "        metadata[\"organization\"] = organization\n",
        "        metadata[\"title\"] = title\n",
        "        if metadata.get(\"page\", None) is not None:\n",
        "            metadata[\"page\"] += 1\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "def explore_documents(documents):\n",
        "    block_indent = \"   \"\n",
        "    metadata = documents[0].metadata\n",
        "    content = documents[0].page_content[:300] + \". . .\"\n",
        "    print(f\"{metadata['organization']} {metadata['title']} {len(documents)} chunks\")\n",
        "    print(\"Truncated First chunk:\")\n",
        "    print(\n",
        "        textwrap.fill(\n",
        "            content,\n",
        "            initial_indent=block_indent,\n",
        "            subsequent_indent=block_indent,\n",
        "            replace_whitespace=True,\n",
        "        )\n",
        "    )\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmTRjEAzGKyq",
        "outputId": "e0353cd9-ab80-404b-d367-b454127bad42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple Privacy Policy 18 chunks\n",
            "Truncated First chunk:\n",
            "   Apple Privacy Policy Appleâ€™s Privacy Policy describes how Apple\n",
            "   collects, uses, and shares your personal data. Updated December 22,\n",
            "   2022 In addition to this Privacy Policy, we provide data and\n",
            "   privacy information embedded in our products and certain features\n",
            "   that ask to use your personal data. This . . .\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohere Privacy Policy 10 chunks\n",
            "Truncated First chunk:\n",
            "   Products  For Developers  For Business  Pricing  Blog  Company  Try\n",
            "   now  Cohere Privacy Policy  Last Update: Aug 4, 2023  Cohere Inc.\n",
            "   (â€œCohereâ€) values and respects your privacy. We have prepared this\n",
            "   privacy policy to explain the manner in which we collect, use, and\n",
            "   disclose personal information th. . .\n",
            "\n",
            "Google Privacy Policy 20 chunks\n",
            "Truncated First chunk:\n",
            "   Privacy Policy Last modified: December 18, 2017 ( view archived\n",
            "   versions ) (The hyperlinked examples are available at the end of\n",
            "   this document.) There are many different ways you can use our\n",
            "   services â€“ to search for and share information, to communicate with\n",
            "   other people or to create new content. Wh. . .\n",
            "\n",
            "Hugging Face Privacy Policy 12 chunks\n",
            "Truncated First chunk:\n",
            "   Terms of Service  Privacy Policy  Content Policy  Code of Conduct\n",
            "   Hugging Face Privacy Policy  ðŸ—“ Effective Date: March 28, 2023  We\n",
            "   have implemented this Privacy Policy because your privacy is\n",
            "   important to us. This Privacy Policy (the â€œPolicyâ€) describes the\n",
            "   type of information that Hugging Face, I. . .\n",
            "\n",
            "Meta Privacy Policy 48 chunks\n",
            "Truncated First chunk:\n",
            "   July 2022   Privacy within Metaâ€™s   Integrity Systems   Why user\n",
            "   rights are at the center   of our safety and security approach. . .\n",
            "\n",
            "Threads Privacy Policy 13 chunks\n",
            "Truncated First chunk:\n",
            "   ðŸ¤Legal  Privacy Policy  Effective date: April 17, 2023  At Threads,\n",
            "   we take your privacy seriously. Please read this Privacy Policy to\n",
            "   learn how we treat your personal data.Â By using or accessing\n",
            "   Threads in any manner, you acknowledge that you accept the\n",
            "   practices and policies outlined below, and yo. . .\n",
            "\n",
            "TikTok Privacy Policy 19 chunks\n",
            "Truncated First chunk:\n",
            "   U.S.  Privacy Policy  Last updated: May 22, 2023  This Privacy\n",
            "   Policy applies to TikTok services (the â€œPlatformâ€), which include\n",
            "   TikTok apps, websites, software and related services accessed via\n",
            "   any platform or device that link to this Privacy Policy. The\n",
            "   Platform is provided and controlled by TikTo. . .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chunks = []\n",
        "\n",
        "for row in policy_df.itertuples(index=False):\n",
        "    policy_chunks = get_chunks(row.url, row.organization, row.title)\n",
        "    explore_documents(policy_chunks)\n",
        "    chunks += policy_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQq92zFXYgas",
        "outputId": "0d5b200d-ced9-4b5b-9547-2884ecb2a7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 140 chunks.\n"
          ]
        }
      ],
      "source": [
        "print (f\"There are {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpHwbcryb60a",
        "outputId": "69e75a87-4d04-40d0-cfa1-d365b8ef5ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.46 s, sys: 1.04 s, total: 6.5 s\n",
            "Wall time: 10.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "collection_name = \"my_collection\"\n",
        "\n",
        "vectordb = Qdrant.from_documents(\n",
        "    documents = chunks,\n",
        "    embedding = embeddings_model,\n",
        "    location = \":memory:\",\n",
        "    collection_name = collection_name\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm that we have the same number of vectors in the vector database as we have chunks.\n",
        "\n",
        "assert vectordb.client.get_collection(collection_name).vectors_count == len(chunks)"
      ],
      "metadata": {
        "id": "CnpTzYi1dXVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQR8k6piWmH5"
      },
      "source": [
        "# Load Persistent Vector Database\n",
        "\n",
        "**If you saved you Qdrant database when you worked on the vector database notebook you can use this section instead of the one that came before.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = \"my_collection\"\n",
        "qdrant_database_location = \"/content/drive/MyDrive/my_qdrant\"\n",
        "\n",
        "client = qdrant_client.QdrantClient(path=qdrant_database_location)\n",
        "\n",
        "vectordb = Qdrant(client=client,\n",
        "                   collection_name=collection_name,\n",
        "                   embeddings=embeddings_model,)\n",
        "\n",
        "#assert vectordb.client.get_collection(collection_name).vectors_count == len(chunks)"
      ],
      "metadata": {
        "id": "n3a24u_Atudm",
        "outputId": "844eb48b-911f-4e8e-c4d3-58a4fb8eb5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBlockingIOError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/portalocker/portalocker.py\u001b[0m in \u001b[0;36mlock\u001b[0;34m(file_, flags)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mfcntl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mlocking_exceptions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBlockingIOError\u001b[0m: [Errno 11] Resource temporarily unavailable",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLockException\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qdrant_client/local/qdrant_local.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             portalocker.lock(\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flock_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/portalocker/portalocker.py\u001b[0m in \u001b[0;36mlock\u001b[0;34m(file_, flags)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# every IO error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLockException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLockException\u001b[0m: [Errno 11] Resource temporarily unavailable",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-8fedcc3bd65e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mqdrant_database_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/my_qdrant\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqdrant_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQdrantClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqdrant_database_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m vectordb = Qdrant(client=client, \n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qdrant_client/qdrant_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, location, url, port, grpc_port, prefer_grpc, https, api_key, prefix, timeout, host, path, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQdrantLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qdrant_client/local/qdrant_local.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, location)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maliases\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flock_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextIOWrapper\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qdrant_client/local/qdrant_local.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mportalocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLockException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;34mf\"Storage folder {self.location} is already accessed by another instance of Qdrant client.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;34mf\" If you require concurrent access, use Qdrant server instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Storage folder /content/drive/MyDrive/my_qdrant is already accessed by another instance of Qdrant client. If you require concurrent access, use Qdrant server instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE_H-WMCRLsi"
      },
      "source": [
        "# Prompting a Model\n",
        "\n",
        "Before we introduce working with the vector database, let's experiment with some simple model prompts.  We will pass a string to Cohere's `command` model, which is its default generative model, and see how it responds.  We will do the same thing with OpenAI's `chatgpt-3.5`.\n",
        "\n",
        "The responses are based on information the model was trained on.  We don't know if it they are accurate.  Sources are not presented.  The Cohere response for Threads doesn't seem to relate to the Threads social media platform and the OpenAI model isn't able to fine Threads information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ypdFdVcFzIZ"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "cohere_llm = Cohere(model=\"command\", temperature=0)\n",
        "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF3hstmLJy4T",
        "outputId": "3910b843-f6fd-4b1a-82c6-6624fcda4fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple has a strong commitment to protecting the privacy of its\n",
            "customers. The company does not sell personal data to third parties.\n",
            "However, it does collect and use data to provide services and improve\n",
            "products.  For example, Apple collects data about how people use their\n",
            "devices, such as which apps are used and how often. This data is used\n",
            "to improve the user experience and develop new features.  Apple also\n",
            "collects data about customers' purchasing habits, which is used to\n",
            "improve marketing and develop new products.  In addition, Apple\n",
            "collects data about customers' location, which is used to provide\n",
            "location-based services and improve maps.  Apple's commitment to\n",
            "privacy is a key selling point for many customers. The company's\n",
            "privacy policies are designed to protect customers' personal data and\n",
            "ensure that it is used only for legitimate purposes.\n"
          ]
        }
      ],
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "result = cohere_llm(query)\n",
        "print(textwrap.fill(result.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_cJGFZcKmTa",
        "outputId": "be1ee238-5162-4ab4-8e5f-d12353c59b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threads does not sell your personal data. We are committed to\n",
            "protecting your privacy and will never share your personal information\n",
            "with third parties without your permission.  We may collect and use\n",
            "your personal information for the following purposes:  - To provide\n",
            "you with the services you request - To improve our services and\n",
            "products - To communicate with you about our services and products -\n",
            "To protect our rights and property - To comply with legal obligations\n",
            "We will never share your personal information with third parties\n",
            "without your permission, except as required by law. We will never sell\n",
            "your personal information to third parties.  If you have any questions\n",
            "or concerns about how we use your personal information, please contact\n",
            "us at support@threadscanada.com.\n"
          ]
        }
      ],
      "source": [
        "query = \"Does Threads sell my personal data?\"\n",
        "result = cohere_llm(query)\n",
        "print(textwrap.fill(result.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdxSkhYzRIn7",
        "outputId": "d0321b45-1ca3-4052-b0d4-afc755d9e101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple has a strong commitment to privacy and has stated that it does\n",
            "not sell personal data to third parties. Apple's business model\n",
            "primarily relies on selling hardware, software, and services rather\n",
            "than monetizing user data. However, it is important to note that Apple\n",
            "does collect and use certain user data for various purposes, such as\n",
            "improving its products and services, but it is typically anonymized\n",
            "and aggregated to protect user privacy.\n"
          ]
        }
      ],
      "source": [
        "messages = [HumanMessage(content=\"Does Apple sell my personal data?\")]\n",
        "print(textwrap.fill(openai_llm(messages).content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jDHiZvfSKNr",
        "outputId": "30a8dce7-9b98-4ee3-e4f5-03ad1ca52ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI language model, I don't have access to specific company\n",
            "policies or practices. However, it is important to note that I am\n",
            "developed by OpenAI and designed to respect user privacy and\n",
            "confidentiality. My primary function is to provide information and\n",
            "answer questions to the best of my knowledge and abilities. If you\n",
            "have concerns about data privacy, it is recommended to review the\n",
            "privacy policy of Threads or contact the company directly for more\n",
            "information.\n"
          ]
        }
      ],
      "source": [
        "messages = [HumanMessage(content=\"Does Threads sell my personal data?\")]\n",
        "print(textwrap.fill(openai_llm(messages).content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4ilSV7wWIjL"
      },
      "source": [
        "# LangChain PromptTemplate\n",
        "\n",
        "A prompt template is a reproducible way to generate prompts. It's essentially a text string that can take in a set of parameters from the end user and generate a prompt accordingly.  Let's shift to LangChain chains by using the simplest of templates.  In these examples, we use the large language model you selected above.  Remember, the model is generating responses based on its training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAZZ5S7CXKPy"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbFtY8wIX52u"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Question: {question} Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISu_7hQHWwrr"
      },
      "source": [
        "Let's inspect the prompt included inside the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzlDNyNGWlUD",
        "outputId": "6ad098ba-e7af-4766-981f-5ee1f1156a11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Question: {question} Answer:', template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "chain.prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIwxfOMrgh60",
        "outputId": "a7934906-bc02-43e2-8887-adaf0d2c5bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Apple does not sell your personal data. Apple has a strong\n",
            "commitment to privacy and has implemented various measures to protect\n",
            "user data.\n"
          ]
        }
      ],
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "print(textwrap.fill(chain.run(query)).strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4J0fsbFbSrl",
        "outputId": "c2239739-6cd6-4ded-c5e9-108483cc3e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Threads does not sell your personal data.\n"
          ]
        }
      ],
      "source": [
        "query = \"Does Threads sell my personal data?\"\n",
        "print(textwrap.fill(chain.run(query)).strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-93wn4aJDkc"
      },
      "source": [
        "# LangChain RetrievalQA Chain\n",
        "\n",
        "Now we introduce our vector database and the LangChain Retrieval QA chain, a chain for question answering against a database of information.  We will also supply our own prompt. It would be reassuring if sources were identified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb6S0jtmHaYn"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJJDm0sPJ7Cr"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "Your answer should be as concise as possible and ideally not more than one sentence.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    LLM, retriever=vectordb.as_retriever(), chain_type_kwargs={\"prompt\": prompt}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-2vl8VQUHo7"
      },
      "source": [
        "Before we call the chain, let's inspect our template and the retriever it includes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug71qmj6UOZ6",
        "outputId": "2751c664-0dd9-4e85-a8a8-02ae34e23143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end.\n",
            "Your answer should be as concise as possible and ideally not more than one sentence.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "\n",
            "{context}\n",
            "\n",
            "Question: {question}\n",
            "\n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "print(chain.combine_documents_chain.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnMYjJnkopbe",
        "outputId": "86799b1c-61d7-48b0-e02c-8c3d12a84d69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Qdrant', 'HuggingFaceEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.qdrant.Qdrant object at 0x782d00f33f10>, search_type='similarity', search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "chain.retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeYv1FFgH40Z",
        "outputId": "c17529a6-30dd-4972-e904-71eb70f948a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Apple does not sell your personal data.\n"
          ]
        }
      ],
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "result = chain.run(query)\n",
        "print(textwrap.fill(result.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHdqFIKrjZPC",
        "outputId": "7e14994c-8616-4f34-8105-b2aa824d3827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.\n"
          ]
        }
      ],
      "source": [
        "query = \"Does TikTok sell my personal data?\"\n",
        "result = chain.run(query)\n",
        "print(textwrap.fill(result.strip()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va1v0iw4JVTI"
      },
      "source": [
        "Let's use the same prompt and get source documents too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFnx-RauKDlr"
      },
      "outputs": [],
      "source": [
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm=LLM,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    return_source_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6zq6Y3BJkwx"
      },
      "outputs": [],
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "result = chain(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pU-vcTibowz"
      },
      "source": [
        "Now, instead of returning a string, the result is a dictionary with three keys, `query`, `result` and `source_documents`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utZqTTuKaULt",
        "outputId": "06ed810c-2db7-4d74-fe1c-fc0765b44dfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['query', 'result', 'source_documents'])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "result.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUh4FR_ZJslt",
        "outputId": "674d5aa8-9957-43d2-8064-bb4586413eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Apple does not sell your personal data.\n"
          ]
        }
      ],
      "source": [
        "print(textwrap.fill(result[\"result\"].strip()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CW7zm4dcVY8"
      },
      "source": [
        "Let's examine the `organization` field for the source documents from that result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmPDfqMdPf0j",
        "outputId": "60ffeb5a-8569-4d12-d93e-2d5e677635a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple', 'Apple', 'Apple', 'Apple']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "[source.metadata[\"organization\"] for source in result[\"source_documents\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaDH38OWce2M"
      },
      "source": [
        "Let's ask a general question and then say we only care about Apple and Hugging Face.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67Wxty-5gU6N"
      },
      "outputs": [],
      "source": [
        "query = \"Do companies use cookies?  I only care about Apple and Hugging Face.\"\n",
        "result = chain(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG0YENl3gd69",
        "outputId": "c25fb4bd-b9e3-49ad-85a1-3774d3ca14cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, both Apple and Hugging Face use cookies.\n"
          ]
        }
      ],
      "source": [
        "print(textwrap.fill(result[\"result\"].strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gGuqT-PgiW-",
        "outputId": "8b31103d-37a6-4c71-d142-52db7bf95122"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple', 'Apple', 'Apple', 'Hugging Face']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "[source.metadata[\"organization\"] for source in result[\"source_documents\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask about Microsoft.  Remember, we have not loaded the Microsoft policy.  Notice that we get an answer about Microsoft, which must be based on the model's training data.  Also, our answer about Cohere isn't based on the source documents listed."
      ],
      "metadata": {
        "id": "HHhwHsfJiVZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Do companies use cookies?  I only care about Apple, Cohere and Microsoft.\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "NWMzOzMhidLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(result[\"result\"].strip()))"
      ],
      "metadata": {
        "id": "GOB6Z1ZDiiIr",
        "outputId": "9325cd2f-3c59-4dac-8cd9-33f8e8415717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, companies use cookies, including Apple, Cohere, and Microsoft.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[source.metadata[\"organization\"] for source in result[\"source_documents\"]]"
      ],
      "metadata": {
        "id": "DpYtR5A9i9ct",
        "outputId": "8d0b00db-c523-45c9-d1c3-acdc7cdc997f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple', 'Apple', 'Hugging Face', 'Threads']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6JIwM0QSnRu"
      },
      "source": [
        "# Self-querying retriever.\n",
        "\n",
        "A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying vectorstore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\n",
        "\n",
        "The self-querying retriever's arguments include descriptions of the metadata fields and the document content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmk2YL-RXTqB",
        "outputId": "99f01134-de2a-4b15-ab8b-52720c171b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/108.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.7/108.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hlark has been installed.\n"
          ]
        }
      ],
      "source": [
        "install_if_needed(\"lark\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsS_U-kdS1LY"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "\n",
        "metadata_field_info = [\n",
        "    AttributeInfo(\n",
        "        name=\"organization\",\n",
        "        description=\"The company or organization that created the document.  It describes that company's policy.\",\n",
        "        type=\"string or list[string]\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"title\",\n",
        "        description=\"The title of the document\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"url\",\n",
        "        description=\"The url for the document\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "]\n",
        "document_content_description = \"A policy\"\n",
        "\n",
        "retriever = SelfQueryRetriever.from_llm(\n",
        "    LLM,\n",
        "    vectordb,\n",
        "    document_content_description,\n",
        "    metadata_field_info,\n",
        "    verbose=True,\n",
        "    enable_limit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the `get_relevant_documents` method provided by the `SelfQueryRetriever` class and examine both the metadata filter generated and the organization for the relevant documents retrieved.\n",
        "\n",
        "As you look at the examples below and substitute your own you should discover that the approach is not consistent reliable.  In some cases, the system seems to fail to understand that a word in a query is an organization.  Sometimes revising the prompt a little bit to make that distinction more apparent helps.\n",
        "\n",
        "Does this suggest that using our generative models to do entity extraction is perhaps not the best way to proceed?"
      ],
      "metadata": {
        "id": "Uqm68NxSoZfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next example, the system does not identify Meta as an organization.  Nevertheless, it is interesting to note that three of the four examples relate to Meta and the first example, is about one of Meta's businesses, Threads."
      ],
      "metadata": {
        "id": "mEG3Y3oGquD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXGvcZQG20IK",
        "outputId": "adb104aa-b406-44fe-d836-2290510d1ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:278: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query='Meta protect my data' filter=None limit=None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Threads', 'Meta', 'Meta', 'Meta']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "documents = retriever.get_relevant_documents(\"How does Meta protect my data?\")\n",
        "[document.metadata[\"organization\"] for document in documents]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, when we change the query to make more explicit that we are talking about the company named Meta, the filter we want is generated and the documents are limited to Meta."
      ],
      "metadata": {
        "id": "V-VzKzEWq5-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = retriever.get_relevant_documents(\"How does the company named Meta protect my data?\")\n",
        "[document.metadata[\"organization\"] for document in documents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTIOgeZLqds3",
        "outputId": "5186dc3b-9b66-4c61-f5bb-f1d108f4ab24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query='Meta data protection' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='organization', value='Meta') limit=None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Meta', 'Meta', 'Meta', 'Meta']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the subtle and even more interesting things about LangChain's self query retriever is that we can use it to allow the query to specify the number of documents to fetch.  We did that by passing `enable_limit=True` to the constructor.  See the relevant documentation [here](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/qdrant_self_query).\n",
        "\n",
        "In the next example, the prompt asks for five examples, the query has `limit=5` and we get five results instead of the default of four."
      ],
      "metadata": {
        "id": "lXXH9v6-rsOg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq_8AfDdaQBa",
        "outputId": "78b3752e-7801-4868-d87b-a2d0709273d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query='Meta protect my data' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='organization', value='Meta') limit=5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Meta', 'Meta', 'Meta', 'Meta', 'Meta']"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ],
      "source": [
        "documents = retriever.get_relevant_documents(\"How does the company named Meta protect my data? I want five examples.\")\n",
        "[document.metadata[\"organization\"] for document in documents]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we don't know if the relevant documents say the same thing.  Let's take a look."
      ],
      "metadata": {
        "id": "0eWQrjlRz75P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for document in documents:\n",
        "  print (textwrap.fill(document.page_content))\n",
        "  print()"
      ],
      "metadata": {
        "id": "-lq0kL1vyxt3",
        "outputId": "b569e5ab-15b7-4df7-fae3-09b4525152ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "July 2022   Privacy within Metaâ€™s   Integrity Systems   Why user\n",
            "rights are at the center   of our safety and security approach\n",
            "\n",
            "and integrity issues we see across Meta, 2) what people and\n",
            "governments are asking social   media companies to do on both privacy\n",
            "and safety, and 3) the process where we assess privacy   concerns and\n",
            "ensure adequate protections in tools built for safety.   Meta is\n",
            "committed to reducing bad experiences on our services.\n",
            "\n",
            "The kind of harms and negative experiences that Meta seeks to prevent\n",
            "on our services through   our Community Standards are not new, not\n",
            "unique to the internet, and not unique to Meta.  8   Academics,\n",
            "regulators, and non-profit organizations have been tackling questions\n",
            "of safety   5\n",
            "\n",
            "Privacy is a core value in safety and security enforcement.  5   Meta\n",
            "is committed to reducing bad experiences on our services.  5   The\n",
            "regulatory environment for privacy, free speech, and safety is\n",
            "shifting.  7   Metaâ€™s Privacy Review offers a process to analyze\n",
            "privacy alongside   other safety, security, and integrity concerns.  8\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Those responses seem reasonably distinct.  But what if they were too similar?  This is where the concept of maximum marginal relevance (\"MMR\") is useful.  MMR is used to diversify the results returned by a search algorithm by selecting items that are both relevant to a query and different from each other.  A discussion of MMR is beyond the scope of this notebook.  Moreover, the set of documents we have included consist of single policy from each organization and there is relatively little redundancy."
      ],
      "metadata": {
        "id": "846UZ21a0OxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's ask about two companies.  The filter seems to be doing the right thing.  However, the set of documents returned is limited to just one of the companies. Maybe that is ok since our question asks if either organization uses cookies."
      ],
      "metadata": {
        "id": "G2-TylLItSOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OmkbFstaWmG",
        "outputId": "8c70e7f5-69ad-47cb-a91c-9fb16cf679ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query='cookies' filter=Operation(operator=<Operator.OR: 'or'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='organization', value='Apple'), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='organization', value='Microsoft')]) limit=None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple', 'Apple', 'Apple', 'Apple']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "documents = retriever.get_relevant_documents(\"Do Apple or Microsoft use cookies?\")\n",
        "[document.metadata[\"organization\"] for document in documents]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But when we ask specifically about each company in a single query, the system only retrieves documents about Apple as being relevant.  This is hard.  Our vector database stores information about single documents, none of which reference other companies (ignoring that Threads is a Meta business).  We need a more sophisticated approach."
      ],
      "metadata": {
        "id": "hcubQF97wphn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = retriever.get_relevant_documents(\"Does Apple use cookies?  Does Microsoft use cookies?\")\n",
        "[document.metadata[\"organization\"] for document in documents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwCqV803we9v",
        "outputId": "c0798b73-430c-45b7-f5e7-259184e1cb4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query='cookies' filter=Operation(operator=<Operator.OR: 'or'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='organization', value='Apple'), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='organization', value='Microsoft')]) limit=None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple', 'Apple', 'Apple', 'Apple']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to this point we have not included prompts that looked to exclude information.  Let's try some below.  \n",
        "\n",
        "In the first example, we get responses related to Threads, TikTok and Apple."
      ],
      "metadata": {
        "id": "H-AiXcZE--Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = retriever.get_relevant_documents(\"How do companies protect my data.\")\n",
        "[document.metadata[\"organization\"] for document in documents]"
      ],
      "metadata": {
        "id": "Ro5HG6Hy_TXy",
        "outputId": "0524d452-e655-446a-d5aa-71a2ba9ee186",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query='data protection' filter=None limit=None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Threads', 'Cohere', 'TikTok', 'Cohere']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we revise the prompt to exclude Threads."
      ],
      "metadata": {
        "id": "s1wxgssS_01C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = retriever.get_relevant_documents(\"How do companies protect my data.  I am not interested in information about Threads.\")\n",
        "[document.metadata[\"organization\"] for document in documents]"
      ],
      "metadata": {
        "id": "bBT_PPi19nAi",
        "outputId": "e6fa6f79-e07e-4309-f57b-9ad7cdfb5f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query='data protection' filter=Operation(operator=<Operator.NOT: 'not'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='organization', value='Threads')]) limit=None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cohere', 'TikTok', 'Cohere', 'Apple']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we revise the prompt to exclude both Threads and Apple."
      ],
      "metadata": {
        "id": "MBEVLY2C_62K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = retriever.get_relevant_documents(\"How do companies protect my data.  I do not care about Threads or Apple.\")\n",
        "[document.metadata[\"organization\"] for document in documents]"
      ],
      "metadata": {
        "id": "D_GSVteU9uuv",
        "outputId": "0ac74624-0de1-4ba4-960b-2331adc80ae0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query='data protection' filter=Operation(operator=<Operator.NOT: 'not'>, arguments=[Operation(operator=<Operator.OR: 'or'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='organization', value='Threads'), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='organization', value='Apple')])]) limit=None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cohere', 'TikTok', 'Cohere', 'Meta']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you experiment with the self query mechanism you will likely conclude that it is fragile.  The implementations seem to vary by vector database as well.  For example, when this notebook was created, Chroma, a popular vector database for simple LangChain examples, does not support self query operations that result in the use of the `NOT` operator even though the query parser recognizes when that operator should be used and Chroma supports that operator directly."
      ],
      "metadata": {
        "id": "kmkaOXhSACHC"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zhQ4p1pSkFfc"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zhQ4p1pSkFfc"
      ],
      "authorship_tag": "ABX9TyMTMDn5fsOtqWQC/ES7IdUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RERobbins/data_science_266_sandbox/blob/main/Embeddings_and_Vector_Databases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "zhQ4p1pSkFfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI and Cohere Accounts and API Endpoints\n",
        "\n",
        "OpenAI and Cohere provide large language models.  This notebook uses API endpoints from each.  \n",
        "\n",
        "OpenAI provides free, rate-limited usage to its API endpoint for a three-month trial period.  In order to use the OpenAI API endpoint you will need an OpenAI API token.\n",
        "\n",
        "Cohere provides free, rate-limited usage for learning and prototyping. In order to use Cohere you will need a Cohere API token.\n",
        "\n",
        "If you don't already have an OpenAI API token you can get one by signing up for a free OpenAI account from the [OpenAI Signup page](https://platform.openai.com/signup?launch).  Once you have created your OpenAI account you can create a trial API key from the [OpenAI API page](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "If you don't already have a Cohere API token you can get one by signing up for a Cohere account from the [Cohere dashboard](https://dashboard.cohere.ai).  Once you have created your Cohere account you can create a trial API key from the [Cohere API Keys page](https://dashboard.cohere.ai/api-keys)."
      ],
      "metadata": {
        "id": "fctD29GtpDas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "By default, the data you create in Google Colaboratory does not persist from session to session.  Each session runs in a virtual machine and when that machine goes away, so does your data.  If you want your data to persist, you must store it outside the virtual machine. Google Drive can be used for that purpose.  We use it later in this notebook to store the OpenAI and Cohere API keys."
      ],
      "metadata": {
        "id": "teyGc1Hox8Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWg8VEQ55bc7",
        "outputId": "2abde750-65b4-45e2-ab1d-176f3c8a99a3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python-dotenv\n",
        "\n",
        "`Python-dotenv` is a Python module that allows you to specify environment variables in traditional UNIX-like “.env” (dot-env) file within your Python project directory.\n",
        "\n",
        "Environment variables are key-value pairs for the current user environment. They are generally set by the operating system and the current user-specific configurations.\n",
        "\n",
        "`Python-dotenv` allows the user to work with API keys without exposing them to the outside world."
      ],
      "metadata": {
        "id": "kkumrUT0qR4F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-yoDbPoXiQiQ"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add API Keys to .env File\n",
        "\n",
        "The format of a .env file is simple: each line contains a single key-value pair, with the key and value separated by an equal sign (=). Here's an example:\n",
        "```\n",
        "DATABASE_URL=your_database_url\n",
        "API_KEY=your_api_key\n",
        "DEBUG=True\n",
        "```\n",
        "To use OpenAI and Cohere, we need to set the `OPENAI_API_KEY` and `COHERE_API_KEY` environment variables, which means the `.env` file should have lines that looks like:\n",
        "```\n",
        "OPENAI_API_KEY=your_openai_api_key_value\n",
        "COHERE_API_KEY=your_cohere_api_key_value\n",
        "```\n",
        "One way to do that is to create or edit the `.env` file directly.  As with most Unix systems, file names that begin with a `.` in Google Drive are hidden from display, but you can still access them. For example, if you enter the name `.env` in the Google Drive search bar, you will be able to access your `.env` file (if it exists).\n",
        "\n",
        "You can use the `append_key_to_env` helper function included below.  If used without arguments it will prompt the user for the value of an OpenAI API key and then the value of a Cohere API key and append each with the correct key name to the `.env` file in the root of the users Google Drive folder.  It will create the `.env` file if necessary.  You can provide an alternate list of key names and an alternate path to the function if you desire, but the defaults are fine for this notebook.\n",
        "\n",
        "Once the OpenAI and Cohere APIs key have been stored in the `.env` file you should not need to do anything else with that file.  If either of your keys change you will need to update the `.env` file."
      ],
      "metadata": {
        "id": "0Aq41CsiDe46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def append_key_to_env (env_file_path = '/content/drive/MyDrive/.env',\n",
        "                           key_names = ['OPENAI_API_KEY', 'COHERE_API_KEY']):\n",
        "  import getpass\n",
        "  with open(env_file_path, 'a') as env_file:\n",
        "    for key_name in key_names:\n",
        "      key_value = getpass.getpass (f\"Please enter the value for {key_name}: \")\n",
        "      env_file.write(f'{key_name}={key_value}\\n')"
      ],
      "metadata": {
        "id": "dzqM4pUXrl3E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the # character in the line below and run if desired\n",
        "# append_key_to_env()"
      ],
      "metadata": {
        "id": "XsnAwPn1GFG6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the API Keys\n",
        "\n",
        "The following cell loads the OpenAI and Cohere API keys into the current environment."
      ],
      "metadata": {
        "id": "MnjsPtWyOzoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv('/content/drive/MyDrive/.env')\n",
        "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "COHERE_API_KEY = os.environ[\"COHERE_API_KEY\"]"
      ],
      "metadata": {
        "id": "MyC87ry57rg_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain\n",
        "\n",
        "LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
        "\n",
        "The examples in this notebook use LangChain rather than the underlying Python modules.  A description of LangChain is beyond the scope of this notebook."
      ],
      "metadata": {
        "id": "hKu1ACMquIIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain"
      ],
      "metadata": {
        "id": "e_qMdGLwvG_6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "\n",
        "We will use OpenAI and Cohere large language models and embeddings, calculate cosine similarity and look at some heatmaps.\n",
        "\n",
        "An overview of OpenAI models can be found [here](https://platform.openai.com/docs/models/overview) and an overciew of OpenAI embeddings can be found [here](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings).\n",
        "\n",
        "An overview of Cohere models and embeddings can be found [here](https://docs.cohere.com/docs/models)."
      ],
      "metadata": {
        "id": "GVNMF4FR-Up3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet openai cohere tiktoken\n",
        "\n",
        "import openai\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings.cohere import CohereEmbeddings\n",
        "\n",
        "import cohere\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import Cohere\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "8GTI7DbH-4E-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def openai_token_count (text):\n",
        "  embedding_model = OpenAIEmbeddings()\n",
        "  openai_encoding = tiktoken.encoding_for_model(embedding_model.model)\n",
        "  return len(openai_encoding.encode (text))\n",
        "\n",
        "def cohere_token_count (text, model_name=\"command\"):\n",
        "  co = cohere.Client(COHERE_API_KEY)\n",
        "  return len(co.tokenize(text=text, model=model_name))\n",
        "\n",
        "def embeddings_similarity_heatmap(embeddings, figsize=None):\n",
        "  if figsize is None:\n",
        "    figsize = (len(embeddings), len(embeddings))\n",
        "  similarity_matrix = cosine_similarity(embeddings)\n",
        "  plt.figure(figsize=figsize)\n",
        "  sns.heatmap(cosine_similarity (embeddings), cmap='viridis', annot=True, fmt=\".3f\")"
      ],
      "metadata": {
        "id": "aeXEJmAL2goM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate the embededing models.  \n",
        "\n",
        "The default OpenAI model is `text-embedding-ada-002`, which is the preferred OpenAI embedding model for its GPT 3.5 and GPT 4 models.  \n",
        "The maximum number of tokens for the OpenAI embedding model is 8192.\n",
        "\n",
        "The default Cohere model is `embed-english-v2.0`  \n",
        "The maximum number of tokens for the Cohere embedding model is `512`."
      ],
      "metadata": {
        "id": "aMf9o9-V3tP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_embeddings_model = OpenAIEmbeddings ()\n",
        "openai_embeddings_model.model"
      ],
      "metadata": {
        "id": "quFVz4XwBWK_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "94831189-76bf-436f-a845-a3825d8ed329"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'text-embedding-ada-002'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cohere_embeddings_model = CohereEmbeddings(truncate='None')\n",
        "cohere_embeddings_model.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fzglJXWOzwlS",
        "outputId": "f167916b-8fad-43c9-d861-6cd8d34da709"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'embed-english-v2.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Short Strings"
      ],
      "metadata": {
        "id": "5ifXLy6r7Eh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create OpenAI and Cohere embeddings from a series of strings, calculate pairwise cosine similarity and look at some heatmaps of the results.\n",
        "\n",
        "Which embedding model do you think performs better on these strings?"
      ],
      "metadata": {
        "id": "kmDAL5YX4_w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings = [\"That movie was great.\",\n",
        "           \"That film was wonderful.\",\n",
        "           \"That movie was awesome.\",\n",
        "           \"That film was rotten.\",\n",
        "           \"That show was garbage.\",\n",
        "           \"Grab a snapshot of the trash\",\n",
        "           \"Take a picture of that rubbish.\",\n",
        "           \"Film the garbage.\"]\n",
        "\n",
        "openai_embeddings = openai_embeddings_model.embed_documents(strings)\n",
        "cohere_embeddings = cohere_embeddings_model.embed_documents(strings)"
      ],
      "metadata": {
        "id": "hp2HHpRPBZoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[openai_token_count(string) for string in strings]"
      ],
      "metadata": {
        "id": "Fk3X1FwyVNrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[cohere_token_count(string) for string in strings]"
      ],
      "metadata": {
        "id": "wBG8jW2WvzDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_similarity_heatmap (openai_embeddings)"
      ],
      "metadata": {
        "id": "LgwvQhInVwy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_similarity_heatmap (cohere_embeddings)"
      ],
      "metadata": {
        "id": "NTxh1bAow5Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilingual\n",
        "\n",
        "We use embeddings to capture semantic similarity.  What happens when we look at the same text in several languages.  Will our embeddings reflect the semantic similarity?\n",
        "\n",
        "Let's examine \"Mary had a little lamb\" in English, French and Spanish."
      ],
      "metadata": {
        "id": "rhsS6t0f6Td1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mary_english = \"\"\"Mary had a little lamb,\n",
        "Little lamb, little lamb,\n",
        "Mary had a little lamb,\n",
        "Its fleece was white as snow\n",
        "\n",
        "Everywhere that Mary went,\n",
        "Mary went, Mary went,\n",
        "Everywhere that Mary went\n",
        "The lamb was sure to go.\n",
        "\n",
        "It followed her to school one day\n",
        "School one day, school one day\n",
        "It followed her to school one day\n",
        "Which was against the rules.\n",
        "\n",
        "It made the children laugh and play,\n",
        "Laugh and play, laugh and play,\n",
        "It made the children laugh and play\n",
        "To see the lamb at school\n",
        "\n",
        "And so the teacher turned it out,\n",
        "Turned it out, turned it out,\n",
        "And so the teacher turned it out,\n",
        "But still it lingered near\n",
        "\"\"\"\n",
        "\n",
        "mary_french = \"\"\"Mary avait un petit agneau,\n",
        "Un petit agneau, un petit agneau,\n",
        "Mary avait un petit agneau,\n",
        "C'est toison était blanche comme neige, ouais\n",
        "\n",
        "Partout, les Mary est allé,\n",
        "Mary est allé, Mary est allé\n",
        "Partout, les Mary est allé\n",
        "Le petit agneau était sûr d'aller, ouais\n",
        "\n",
        "Il la suivit à l'école un jour\n",
        "A l'école un jour, à l'école un jour\n",
        "Il la suivit à l'école un jour\n",
        "Et enfreint la règle des enseignants.\n",
        "\n",
        "Il a fait les enfants rient et jouent,\n",
        "Rire et jouer, rire et jouer,\n",
        "Il a fait les enfants rient et jouent\n",
        "Pour voir un agneau à l'école\n",
        "\n",
        "Et si l'enseignant il s'est avéré,\n",
        "Il s'est avéré, il s'est avéré,\n",
        "Et si l'enseignant il s'est avéré,\n",
        "Mais toujours il resta près de\n",
        "\"\"\"\n",
        "\n",
        "mary_spanish = \"\"\"Mary tenía un pequeño cordero,\n",
        "Pequeño cordero, pequeño cordero\n",
        "Mary tenía un pequeño cordero\n",
        "Su lana era blanca como la nieve como la nieve blanca.\n",
        "\n",
        "A cualquier lado que Mary iba\n",
        "Mary iba, Mary iba\n",
        "A cualquier lado que Mary iba\n",
        "El cordero atrás yendo iba\n",
        "\n",
        "Un día a la escuela la siguió\n",
        "A la escuela un día, a la escuela un día\n",
        "Un día a la escuela la siguió\n",
        "Y eso estaba en contra de las reglas de la misma.\n",
        "\n",
        "Hizó a los niños reír y jugar,\n",
        "Reír y jugar, reír y jugar\n",
        "Hizó a los niños reír y jugar\n",
        "De verlo en la escuela estar.\n",
        "\n",
        "Y entonces la maestra lo alejó,\n",
        "Lo alejó, lo alejó\n",
        "Y entonces la maestra lo alejó\n",
        "Pero sin embargo cerca permanecía.\n",
        "\"\"\"\n",
        "\n",
        "mary = [mary_english, mary_french, mary_spanish]\n",
        "mary_openai_embeddings = openai_embeddings_model.embed_documents(mary)\n",
        "mary_cohere_embeddings = cohere_embeddings_model.embed_documents(mary)"
      ],
      "metadata": {
        "id": "w6XlFWtDWhf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[openai_token_count(string) for string in mary]"
      ],
      "metadata": {
        "id": "V8xZEn6M6Im8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[cohere_token_count(string) for string in mary]"
      ],
      "metadata": {
        "id": "hE9-YV1q6Iwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OpenAI embeddings for the three renditions are similar."
      ],
      "metadata": {
        "id": "BsEhUXxT9bYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_similarity_heatmap (mary_openai_embeddings)"
      ],
      "metadata": {
        "id": "hYcM1WmZY0Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Cohere embeddings are not similar."
      ],
      "metadata": {
        "id": "xKsCB6au9n12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_similarity_heatmap (mary_cohere_embeddings)"
      ],
      "metadata": {
        "id": "HYcx3_FrPi1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cohere has a separate embedding model that provides multilingual embedding suport.  Unlike the `embed-english-v2.0` model which has a 512 token maximum, the `embed-multilingual-v2.0` model has a 256 token maximum.  See [Cohere multilingual language models](https://docs.cohere.com/docs/multilingual-language-models)\n",
        "\n",
        "The default Cohere embedding model is, by intent, English only.  The Cohere multilingual model is much stronger on this example.\n",
        "\n",
        "Consider how the OpenAI embedding model compares to the Cohere multilingual embedding model."
      ],
      "metadata": {
        "id": "H7Z1hIq99-9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cohere_multilingual_embeddings_model = CohereEmbeddings(model=\"embed-multilingual-v2.0\", truncate='None')\n",
        "cohere_multilingual_embeddings_model"
      ],
      "metadata": {
        "id": "zn_hXAhr-t02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mary_cohere_multilingual_embeddings = cohere_multilingual_embeddings_model.embed_documents(mary)\n",
        "embeddings_similarity_heatmap (mary_cohere_multilingual_embeddings)"
      ],
      "metadata": {
        "id": "Aeyl38Lx_Yi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add \"Three Blind Mice\", in English and compare the OpenAI and Cohere multilingual embeddings"
      ],
      "metadata": {
        "id": "qutaBB8GClzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mice = \"\"\"Three blind mice, three blind mice\n",
        "See how they run, see how they run\n",
        "They all run away from the farmer's wife\n",
        "Then into a space ship to take a ride\n",
        "Did you ever see such a sight in your life\n",
        "As three blind mice?\n",
        "\n",
        "Three blind mice, three blind mice\n",
        "Crash on the moon, crash on the moon\n",
        "The moon was covered in cheese so high\n",
        "And nobody knew the reason why\n",
        "Did you ever see such a sight in your life\n",
        "As three blind mice?\n",
        "\n",
        "Three blind mice, three blind mice\n",
        "With full tummies, with full tummies\n",
        "All of them ate too much cheese that night\n",
        "With little moon left after every bite\n",
        "Did you ever see such a sight in your life\n",
        "As three blind mice?\"\"\""
      ],
      "metadata": {
        "id": "FqjX-_c4dyXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_token_count(mice)"
      ],
      "metadata": {
        "id": "8LPToKPh6rnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cohere_token_count(mice)"
      ],
      "metadata": {
        "id": "K2MuDKd-6wOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mary_mice_openai_embeddings = openai_embeddings_model.embed_documents(mary + [mice])\n",
        "embeddings_similarity_heatmap (mary_mice_openai_embeddings)"
      ],
      "metadata": {
        "id": "gY_5k_Rtd9h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mary_mice_cohere_multilingual_embeddings = cohere_multilingual_embeddings_model.embed_documents(mary + [mice])\n",
        "embeddings_similarity_heatmap (mary_mice_cohere_multilingual_embeddings)"
      ],
      "metadata": {
        "id": "YzYHd-8OROFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens when we add some music lyrics to the mix."
      ],
      "metadata": {
        "id": "gR6odd_aESUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bohemian = \"\"\"I see a little silhouetto of a man\n",
        "Scaramouche, Scaramouche, will you do the Fandango?\n",
        "Thunderbolt and lightning, very, very frightening me\n",
        "(Galileo) Galileo, (Galileo) Galileo, Galileo Figaro, magnifico\n",
        "But I'm just a poor boy, nobody loves me\n",
        "He's just a poor boy from a poor family\n",
        "Spare him his life from this monstrosity\"\"\""
      ],
      "metadata": {
        "id": "dZmEI0FKETMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_token_count(bohemian)"
      ],
      "metadata": {
        "id": "ESF2Lz9q62hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cohere_token_count(bohemian)"
      ],
      "metadata": {
        "id": "OIF6P-nC68f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amalgam_openai_embeddings = openai_embeddings_model.embed_documents(mary + [mice, bohemian])\n",
        "embeddings_similarity_heatmap (amalgam_openai_embeddings)"
      ],
      "metadata": {
        "id": "kw-UvItVHbwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amalgam_cohere_multilingual_embeddings = cohere_multilingual_embeddings_model.embed_documents(mary + [mice, bohemian])\n",
        "embeddings_similarity_heatmap (amalgam_cohere_multilingual_embeddings)"
      ],
      "metadata": {
        "id": "dTusrJF8Hkft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Databases"
      ],
      "metadata": {
        "id": "4-zF6E82OES8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "A vector database, also known as a vector search engine or similarity search database, is a type of database that specializes in storing and retrieving high-dimensional vectors efficiently.  The embeddings we have been using are high-dimensional vectors.\n",
        "\n",
        "In the context of question answering tasks, vector databases can be particularly useful for tasks like semantic search, where you want to find documents or data points that are semantically similar to a given query.\n",
        "\n",
        "Traditional relational databases are not well-suited for efficiently querying and retrieving semantically similar data. Vector databases, on the other hand, are designed to handle similarity-based searches efficiently.\n",
        "\n",
        "Vector databases are an essential component of modern natural language processing solutions that are built to apply the generative capabilities of large language models to data collections.  This approach is called retrieval augmented generation or \"RAG\".\n",
        "\n",
        "RAG is used in tasks like question answering.  With RAG, a retrieval component first selects a set of relevant documents or passages from a larger corpus, and then a generation component generates the final response based on the selected information. This approach aims to combine the accuracy of retrieval with the flexibility of generation.\n",
        "\n",
        "We will use Chroma, a simple vector database and explore some of the most important concepts.\n",
        "\n",
        "Up to this point in the notebook we have experimented with several different embedding models.  The embeddings we use with a large language model need to be suitable for use with that model.  For example, the OpenAI embeddings we have used so far are compatible with both `gpt-3.5` and `gpt-4` models.  We have experimented with two different Cohere embedding models, engligh and multi-lingual.  These are compatible with Cohere's current `command` model.\n",
        "\n",
        "From this point on in the notebook, we will stick to a single large language model and a single embedding model.  We will also select a single embedding model.  OpenAI trial accounts expire after three months and provide access to `gpt-3.5` but not `gpt-4`.  Paid OpenAI accounts permit use of `gpt-4` as well and do not expire.  Cohere trial accounts do not expire, but the API rate limiting is more significant than OpenAI trial account rate limiting.\n",
        "\n",
        "If you set `LLM_SOURCE` to `\"OpenAI\"` you will use the `openai_embeddings_model`.  If you set `LLM_SOURCE` to `\"Cohere\"` you need to select whether you want `cohere_embeddings_model` (which accomodates more tokens but it limited to English) or `cohere_multilingual_embeddings_model`.\n",
        "\n",
        "If you set `LLM_SOURCE` to `\"OpenAI\"` and your OpenAI license is a trial license, you will not have access to `gpt-4`.  However, if you have a paid OpenAI license, you can move from `gpt-3.5-turbo` to `gpt-4`.  If you set `LLM_SOURCE` to `\"Cohere\"` you will use Cohere's `command` large language model.\n",
        "\n",
        "The results in most of the examples below will vary with your choices.\n"
      ],
      "metadata": {
        "id": "FDPR34aEOEe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_SOURCE=\"Cohere\"  #set this to either \"OpenAI\" or \"Cohere\"\n",
        "\n",
        "if LLM_SOURCE==\"OpenAI\":\n",
        "  embeddings_model=openai_embeddings_model\n",
        "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "# llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "if LLM_SOURCE==\"Cohere\":\n",
        "  embeddings_model=cohere_embeddings_model\n",
        "# embeddings_model=cohere_multilingual_embeddings_model\n",
        "  llm = Cohere(model=\"command\", temperature=0)"
      ],
      "metadata": {
        "id": "BwggMZsAamu8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet chromadb\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "McW_mfokeGZy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Chunking\n",
        "\n",
        "Document chunking, also known as text segmentation or document splitting, refers to the process of breaking down large documents or pieces of text into smaller, manageable segments before feeding them to large language models. There are several reasons why chunking is important when working with these models.\n",
        "\n",
        "Chunking documents when working with large language models is essential to overcome input limitations, improve performance, manage costs, ensure complete responses, maintain contextual coherence, and guide the model's attention effectively. It allows you to make the most out of these powerful models when dealing with lengthy or complex text documents.\n",
        "\n",
        "In the following code cells, we will download several corporate privay policies from the web.  We will use document loaders specific to `pdf` files or `urls` as the case may be.\n",
        "\n",
        "We the use LangChain's `RecursiveCharacterTextSplitter` to chunk each document.  See the relevant [LangChain documentation](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter).\n",
        "\n",
        "We add a piece of metadata that identifies the relevant organization for each chunk.  We discuss this in greater detail below."
      ],
      "metadata": {
        "id": "afB_K7CtfKUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet pypdf unstructured\n",
        "\n",
        "import textwrap\n",
        "from langchain.document_loaders import PyPDFLoader, UnstructuredURLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "cwfjt4R5hMy6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "policy_data = [\n",
        "    (\"Apple\", \"Privacy Policy\", \"https://www.apple.com/legal/privacy/pdfs/apple-privacy-policy-en-ww.pdf\"),\n",
        "    (\"Google\", \"Privacy Policy\", \"https://static.googleusercontent.com/media/www.google.com/en//intl/en/policies/privacy/google_privacy_policy_en.pdf\"),\n",
        "    (\"Meta\", \"Privacy Policy\", \"https://about.fb.com/wp-content/uploads/2022/07/Privacy-Within-Metas-Integrity-Systems.pdf\"),\n",
        "    (\"TikTok\", \"Privacy Policy\", \"https://www.tiktok.com/legal/page/us/privacy-policy/en\"),\n",
        "    (\"Threads\", \"Privacy Policy\", \"https://terms.threads.com/privacy-policy\")\n",
        "]\n",
        "\n",
        "columns = ['organization', 'title', 'url']\n",
        "\n",
        "policy_df = pd.DataFrame(policy_data, columns=columns)"
      ],
      "metadata": {
        "id": "Ch1pJycHjh0L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks (url, organization, title, chunk_size=1000, chunk_overlap=100):\n",
        "\n",
        "    \"\"\"\n",
        "    This function takes a url to an organization's web page, organization name,\n",
        "    and document title and returns chunks constructed from the target url.\n",
        "    The function adds the url, the organization name and the document title\n",
        "    as metadata to the chunks.\n",
        "\n",
        "    Parameters:\n",
        "    url (string): Target page.\n",
        "    organization (string): Organization name.\n",
        "    title: Document title.\n",
        "    chunk_size (int, optional): Chunk size, default is 1000 characters.\n",
        "    chunk_overlap (int, optional): Chunk overlap, default is 10 characters.\n",
        "\n",
        "    Returns:\n",
        "    list of chunks\n",
        "    \"\"\"\n",
        "\n",
        "    # Use PyPDFLoader for pdf targets, otherwise UnstructuredURLLoader\n",
        "    if os.path.splitext(url)[1] == \".pdf\":\n",
        "      loader = PyPDFLoader(url)\n",
        "    else:\n",
        "      loader = UnstructuredURLLoader(urls=[url])\n",
        "\n",
        "    # Load the documents and add organization metadata field.\n",
        "    # Increment page count metadata by one so it's not zero-based.\n",
        "\n",
        "    documents = loader.load()\n",
        "    for document in documents:\n",
        "      metadata = document.metadata\n",
        "      metadata['url'] = url\n",
        "      metadata['organization'] = organization\n",
        "      metadata['title'] = title\n",
        "      if metadata.get('page', None) is not None:\n",
        "        metadata['page'] += 1\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                                   chunk_overlap=chunk_overlap)\n",
        "\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "def explore_documents (documents):\n",
        "  block_indent = \"   \"\n",
        "  metadata=documents[0].metadata\n",
        "  content=documents[0].page_content[:200] + \". . .\"\n",
        "  print(f\"{metadata['organization']} {metadata['title']} {len(documents)} chunks\")\n",
        "  print(\"Truncated First chunk:\")\n",
        "  print(textwrap.fill(content,\n",
        "                      initial_indent=block_indent,\n",
        "                      subsequent_indent=block_indent,\n",
        "                      replace_whitespace=True))\n",
        "  print()"
      ],
      "metadata": {
        "id": "aVsFWsVbAVFm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks=[]\n",
        "\n",
        "for row in policy_df.itertuples(index=False):\n",
        "  policy_chunks = get_chunks(row.url, row.organization, row.title)\n",
        "  explore_documents(policy_chunks)\n",
        "  chunks += policy_chunks"
      ],
      "metadata": {
        "id": "jmTRjEAzGKyq",
        "outputId": "543dbbc2-5316-4582-c958-29747bc359d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple Privacy Policy 33 chunks\n",
            "Truncated First chunk:\n",
            "   Apple Privacy Policy Apple’s Privacy Policy describes how Apple\n",
            "   collects, uses, and shares your personal data. Updated December 22,\n",
            "   2022 In addition to this Privacy Policy, we provide data and\n",
            "   privacy. . .\n",
            "\n",
            "Google Privacy Policy 32 chunks\n",
            "Truncated First chunk:\n",
            "   Privacy Policy Last modified: December 18, 2017 ( view archived\n",
            "   versions ) (The hyperlinked examples are available at the end of\n",
            "   this document.) There are many different ways you can use our\n",
            "   services . . .\n",
            "\n",
            "Meta Privacy Policy 78 chunks\n",
            "Truncated First chunk:\n",
            "   July 2022   Privacy within Meta’s   Integrity Systems   Why user\n",
            "   rights are at the center   of our safety and security approach. . .\n",
            "\n",
            "TikTok Privacy Policy 41 chunks\n",
            "Truncated First chunk:\n",
            "   How TikTok is supporting our community through COVID-19  U.S.\n",
            "   Privacy Policy  Last updated: May 22, 2023  This Privacy Policy\n",
            "   applies to TikTok services (the “Platform”), which include TikTok\n",
            "   apps, w. . .\n",
            "\n",
            "Threads Privacy Policy 26 chunks\n",
            "Truncated First chunk:\n",
            "   🤝Legal  Privacy Policy  Effective date: April 17, 2023  At Threads,\n",
            "   we take your privacy seriously. Please read this Privacy Policy to\n",
            "   learn how we treat your personal data. By using or accessing Thre.\n",
            "   . .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Vector Database from Chunked Documents\n",
        "\n",
        "We will use the `get_chunks` helper function we used above to assemble a list of all the chunks and then pass that list to a function that instantiates a vector database from those chunks and an embedding model."
      ],
      "metadata": {
        "id": "gQR8k6piWmH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are 210 chunks in all.\n",
        "\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQq92zFXYgas",
        "outputId": "17af2e95-1fe8-44fe-821f-7a4deb39a0f8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "210"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at a couple of chunks.\n",
        "\n",
        "chunks[22]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DkhrnqRaXwi",
        "outputId": "5098a40b-82c9-4e2b-8af4-d7766dc1b4ff"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"To exercise privacy rights for your child's information, visit the Apple Data and Privacy page at privacy.apple.com and sign in to their account. Cookies and Other Technologies Apple’s websites, online services, interactive applications, and advertisements may use “cookies” and other technologies such as web beacons. These technologies help us to better understand user behavior including for security and fraud prevention purposes, tell us which parts of our websites people have visited, and facilitate and measure the effectiveness of advertisements and web searches. •Communications Cookies. These cookies are used to enable network traffic to and from Apple’s systems, including by helping us detect any errors. •Strictly Necessary Cookies. These cookies are set as required to provide a specific feature or service that you have accessed or requested. For example, they allow us to display our websites in the proper format and language, to authenticate and verify your transactions, and to\", metadata={'source': '/tmp/tmpoddw0ra5', 'page': 7, 'url': 'https://www.apple.com/legal/privacy/pdfs/apple-privacy-policy-en-ww.pdf', 'organization': 'Apple', 'title': 'Privacy Policy'})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[128]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2Z226oDai04",
        "outputId": "a116c998-1015-45ec-e253-4628dbfdc103"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Other Policies, Meta Transparency Center,  https://transparency.fb.com/policies/other-policies  . \\n 13.  “Publishing our Internal Enforcement Guidelines and Expanding our Appeal Process,” Meta \\n Newsroom (April 24, 2018), \\n https://about.fb.com/news/2018/04/comprehensive-community-standards/  . \\n 14.  Community Standards, “Bullying and Harassment,” \\n https://transparency.fb.com/policies/community-standards/bullying-harassment/  . \\n 15.  Instagram, Community Guidelines, \\n https://www.facebook.com/help/instagram/477434105621119  . \\n 16. “Since 2016, the strategy of Integrity has been “  remove, reduce, and inform  ” to manage \\n problematic content across the Facebook family of apps. This involves removing content that \\n violates our policies, reducing the spread of problematic content that does not violate our \\n policies and informing people with additional information so they can choose what to click, read', metadata={'source': '/tmp/tmp4pukxhth', 'page': 23, 'url': 'https://about.fb.com/wp-content/uploads/2022/07/Privacy-Within-Metas-Integrity-Systems.pdf', 'organization': 'Meta', 'title': 'Privacy Policy'})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings_model)\n",
        "assert vectordb._collection.count() == len(chunks)"
      ],
      "metadata": {
        "id": "FpHwbcryb60a"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query the Vector Database"
      ],
      "metadata": {
        "id": "8CRD3Ig9fc62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A similarity search that returns the four vectors closest to the query by default."
      ],
      "metadata": {
        "id": "RXpaG3wzko7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "results = vectordb.similarity_search(query)\n",
        "[result.metadata['organization'] for result in results]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzoZVkSKfkrG",
        "outputId": "747d3b33-9971-46e2-961c-c49586a56c9b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple', 'Apple', 'Apple', 'Apple']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine the first result in the list.  It looks to be responsive to the question."
      ],
      "metadata": {
        "id": "2O57ZTQglCRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(results[0].page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofhvMvk4jKRz",
        "outputId": "7ceeedd2-e453-4c7f-f933-43cbc5a2d650"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple’s Sharing of Personal Data Apple may share personal data with\n",
            "Apple-affiliated companies, service providers who act on our behalf,\n",
            "our partners, developers, and publishers, or others at your direction.\n",
            "Apple does not share personal data with third parties for their own\n",
            "marketing purposes.  •Service Providers. Apple may engage third\n",
            "parties to act as our service providers and perform certain tasks on\n",
            "our behalf, such as processing or storing data, including personal\n",
            "data, in connection with your use of our services and delivering\n",
            "products to customers. Apple service providers are obligated to handle\n",
            "personal data consistent with this Privacy Policy and according to our\n",
            "instructions. •Partners. At times, Apple may partner with third\n",
            "parties to provide services or other offerings. For example, Apple\n",
            "financial offerings like Apple Card and Apple Cash are offered by\n",
            "Apple and our partners. Apple requires its partners to protect your\n",
            "personal data. •Developers and Publishers from Whom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the number of results to 10."
      ],
      "metadata": {
        "id": "aO1blNMelaJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "results = vectordb.similarity_search(query, k=10)\n",
        "[result.metadata['organization'] for result in results]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAHwXFKDgVFU",
        "outputId": "fcf2d783-88cd-4ff4-ac50-880a68d47efb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we ask about Apple's use of cookies, there's a problem.  We get responses that don't relate to Apple.\n",
        "\n",
        "Can you imagine why?"
      ],
      "metadata": {
        "id": "egjUC7IYmSBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Apple use cookies?\"\n",
        "results = vectordb.similarity_search (query, k=10)\n",
        "[result.metadata['organization'] for result in results]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCnCCSX-g1vk",
        "outputId": "ee5ef5bb-5796-41d8-87f4-ddffdec66d1d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Google']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first result looks good.  It talks about Apple and the fact that Apple uses cookies."
      ],
      "metadata": {
        "id": "b77GEEPEmyeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(results[0].page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk22v6Ygj0b0",
        "outputId": "ea4ee873-1ac2-44cc-b46d-8dec6ae946ba"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To exercise privacy rights for your child's information, visit the\n",
            "Apple Data and Privacy page at privacy.apple.com and sign in to their\n",
            "account. Cookies and Other Technologies Apple’s websites, online\n",
            "services, interactive applications, and advertisements may use\n",
            "“cookies” and other technologies such as web beacons. These\n",
            "technologies help us to better understand user behavior including for\n",
            "security and fraud prevention purposes, tell us which parts of our\n",
            "websites people have visited, and facilitate and measure the\n",
            "effectiveness of advertisements and web searches. •Communications\n",
            "Cookies. These cookies are used to enable network traffic to and from\n",
            "Apple’s systems, including by helping us detect any errors. •Strictly\n",
            "Necessary Cookies. These cookies are set as required to provide a\n",
            "specific feature or service that you have accessed or requested. For\n",
            "example, they allow us to display our websites in the proper format\n",
            "and language, to authenticate and verify your transactions, and to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets look at any other responses.  Did we ask for too many results?  Are there chunks that talk about cookies but don't mention the company?"
      ],
      "metadata": {
        "id": "c2_chQMXm9UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for result in results:\n",
        "  if result.metadata['organization'] != \"Apple\":\n",
        "    print(\"Organization: \", result.metadata['organization'])\n",
        "    print(textwrap.fill(result.page_content))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voz62_iDkFE2",
        "outputId": "d7d20ea4-2a0f-4ff5-85c0-fc1a644b7552"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Organization:  Google\n",
            "We use information collected from cookies and other technologies, like\n",
            "pixel tags , to improve your user experience  and the overall quality\n",
            "of our services. One of the products we use to do this on our own\n",
            "services is Google Analytics. For example, by saving your language\n",
            "preferences, we’ll be able to have our services appear in the language\n",
            "you prefer. When showing you tailored ads, we will not associate an\n",
            "identifier from cookies or similar technologies with sensitive\n",
            "categories , such as those based on race, religion, sexual orientation\n",
            "or health. Our automated systems analyze your content (including\n",
            "emails) to provide you personally relevant product features, such as\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we processed the source documents and split them into chunks, we added the name of the organization for the policy as metadata.  We can use that metadata as a filter.  In our example below, the filter is very simple, we merely indicate that the organization field needs to be `Apple`.  When we add that parameter, the results are limited to Apple embeddings."
      ],
      "metadata": {
        "id": "W0n2p17Rnrs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vectordb.similarity_search (query, filter = {'organization':'Apple'}, k=10)\n",
        "[result.metadata['organization'] for result in results]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUCwdsfYhD7L",
        "outputId": "50a3ca70-2082-4032-9879-540c1f895af8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple',\n",
              " 'Apple']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next query only references \"the company\" and not any specific company.  The results relate to several of the companies."
      ],
      "metadata": {
        "id": "a_uwtTshoRLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does the company sell private data?\"\n",
        "results = vectordb.similarity_search (query, k=10)\n",
        "[result.metadata['organization'] for result in results]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnXrU-5mifhw",
        "outputId": "19256a27-c33a-4c21-be8c-8799e0aade81"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Threads',\n",
              " 'Google',\n",
              " 'Google',\n",
              " 'TikTok',\n",
              " 'Apple',\n",
              " 'Threads',\n",
              " 'Google',\n",
              " 'TikTok',\n",
              " 'Threads',\n",
              " 'TikTok']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the filter to indicate that we care about Google only."
      ],
      "metadata": {
        "id": "HFZwwynjolpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does the company sell private data?\"\n",
        "results = vectordb.similarity_search (query, filter = {'organization':'Google'}, k=10)\n",
        "[result.metadata['organization'] for result in results]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s1AwRa5iyYe",
        "outputId": "abf4c2a4-6458-44b6-e62f-04aa7b905a40"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Google',\n",
              " 'Google',\n",
              " 'Google',\n",
              " 'Google',\n",
              " 'Google',\n",
              " 'Google',\n",
              " 'Google',\n",
              " 'Google',\n",
              " 'Google',\n",
              " 'Google']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(results[5].page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz6FUkfCi8zI",
        "outputId": "d11effc9-9ea7-48ce-828f-813727c0b652"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "information in order to process it for us, and who are subject to\n",
            "strict contractual confidentiality obligations and may be  disciplined\n",
            "or terminated if they fail to meet these obligations. When this\n",
            "Privacy Policy applies Our Privacy Policy applies to all of the\n",
            "services offered by Google LLC and its affiliates, including YouTube,\n",
            "services Google provides on Android devices, and services offered on\n",
            "other sites (such as our advertising services), but excludes services\n",
            "that have separate privacy policies that do not incorporate this\n",
            "Privacy Policy. Our Privacy Policy does not apply to services offered\n",
            "by other companies or individuals, including products or sites that\n",
            "may be displayed to you in search results, sites that may include\n",
            "Google services, or other sites linked from our services. Our Privacy\n",
            "Policy does not cover the information practices of other companies and\n",
            "organizations who advertise our services, and who may\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answering Questions With Retrieval Augmented Generation\n",
        "\n",
        "Now that we have some familiarity with vector databases, lets use ours, together with a large language model, to answer questions about the five privacy policies we have been working with.  We will continue to rely on the LangChain framework and use Cohere."
      ],
      "metadata": {
        "id": "BtdjwXCzF0tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers\n"
      ],
      "metadata": {
        "id": "nF_lZJIAUYNY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting a Model\n",
        "\n",
        "[reword] Before we introduce working with the vector database, let's experiment with a simple model prompt.  We will pass a string to Cohere's `command` model, which is its default generative model, and see how it responds.\n",
        "\n",
        "The responses are based on information the model was trained on.  We don't know if it they are accurate.  Sources are not presented.  The response for Threads doesn't seem to relate to the Threads social media platform either."
      ],
      "metadata": {
        "id": "KE_H-WMCRLsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Add this section back -- or not."
      ],
      "metadata": {
        "id": "-ypdFdVcFzIZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain PromptTemplate\n",
        "\n",
        "A prompt template is a reproducible way to generate prompts. It's essentially a text string that can take in a set of parameters from the end user and generate a prompt accordingly.  Let's shift to LangChain chains by using the simplest of templates.\n",
        "\n",
        "The responses are not the same as before.  The substance of each has shifted too.\n",
        "\n",
        "Remember, the model is generating responses based on its training data."
      ],
      "metadata": {
        "id": "X4ilSV7wWIjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain"
      ],
      "metadata": {
        "id": "NAZZ5S7CXKPy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question} Answer:\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=['question']\n",
        ")\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "tbFtY8wIX52u"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "print(textwrap.fill(chain.run(query)).strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIwxfOMrgh60",
        "outputId": "23e2bf64-1454-4ed1-bfc1-f6f49406dd07"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple does not sell your personal data. However, it does collect and\n",
            "use data in ways that may be considered a sale under certain laws. For\n",
            "example, Apple may share your data with third parties for marketing\n",
            "purposes, or it may use your data to improve its products and\n",
            "services. In these cases, Apple may be considered to have sold your\n",
            "data even though it does not receive any direct financial\n",
            "compensation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Threads sell my personal data?\"\n",
        "print(textwrap.fill(chain.run(query)).strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4J0fsbFbSrl",
        "outputId": "406fc842-d566-416c-96fb-786f9076e121"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threads does not sell your personal data. Threads is a social media\n",
            "platform that allows users to share their personal experiences and\n",
            "connect with others. Threads does not share or sell user data to third\n",
            "parties.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain RetrievalQA Chain\n",
        "\n",
        "Now we introduced our vector database and the LangChain Retrieval QA chain, a chain for question answering against a database of information.  This time, the response for Apple mentions the privacy policy.  It would be reassuring if sources were identified."
      ],
      "metadata": {
        "id": "8-93wn4aJDkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever()\n",
        "    )"
      ],
      "metadata": {
        "id": "Tb6S0jtmHaYn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "result = chain.run(query)\n",
        "print(textwrap.fill(result.strip()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeYv1FFgH40Z",
        "outputId": "58f7e559-8ef0-497a-e5da-9ce7f6a599c0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Apple does not sell your personal data. Apple's privacy policy\n",
            "explicitly states that they do not share personal data with third\n",
            "parties for their own marketing purposes. However, it is important to\n",
            "note that Apple may share personal data with Apple-affiliated\n",
            "companies, service providers, partners, developers, and publishers, or\n",
            "others at your direction. Additionally, Apple may disclose information\n",
            "about you if required by law or if there is a lawful basis for doing\n",
            "so.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Threads sell my personal data?\"\n",
        "result = chain.run(query)\n",
        "print(textwrap.fill(result.strip()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHdqFIKrjZPC",
        "outputId": "61664c2b-c1af-430c-9870-8b28de9e1659"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Threads does not sell your personal data. The company has a\n",
            "privacy policy in place that outlines how they collect, use, and share\n",
            "personal data. The policy states that they will not share your\n",
            "personal data with third parties unless it is necessary to provide and\n",
            "improve their services, comply with applicable law, or as part of a\n",
            "merger, acquisition, or sale of assets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build our own prompt to be used in the QA chain.  Our prompt will be based on the PromptTemplate we looked at above."
      ],
      "metadata": {
        "id": "1kmXYXJ1lLT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "Your answer should be as concise as possible and not more than three sentences.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"context\", \"question\"]\n",
        "                        )\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")"
      ],
      "metadata": {
        "id": "JJJDm0sPJ7Cr"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "result = chain.run(query)\n",
        "print(textwrap.fill(result.strip()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqWI5rGwvM-t",
        "outputId": "cc9b0389-1d0e-48c0-e08e-1cfc3b9be219"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple does not sell your personal data, including as \"sale\" is defined\n",
            "in Nevada and California. Apple also does not \"share\" your personal\n",
            "data as that term is defined in California.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Threads sell my personal data?\"\n",
        "result = chain.run(query)\n",
        "print(textwrap.fill(result.strip()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohODP11xml4T",
        "outputId": "dcf7d424-11d3-43fb-920c-3e25b5261415"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.cohere:Retrying langchain.llms.cohere.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised CohereAPIError: You are using a Trial key, which is limited to 5 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions.\n",
            "WARNING:langchain.llms.cohere:Retrying langchain.llms.cohere.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised CohereAPIError: You are using a Trial key, which is limited to 5 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions.\n",
            "WARNING:langchain.llms.cohere:Retrying langchain.llms.cohere.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised CohereAPIError: You are using a Trial key, which is limited to 5 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions.\n",
            "WARNING:langchain.llms.cohere:Retrying langchain.llms.cohere.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised CohereAPIError: You are using a Trial key, which is limited to 5 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threads does not sell your personal data. However, Threads may share\n",
            "your personal data with third parties for business purposes, such as\n",
            "providing customer support, analyzing how you interact and engage with\n",
            "Threads, or obtaining information to generate leads and create user\n",
            "profiles. Threads may also share your personal data with vendors,\n",
            "social media networks, and other third parties for commercial or\n",
            "business purposes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get source documents."
      ],
      "metadata": {
        "id": "va1v0iw4JVTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 30}), \\\n",
        "                                 return_source_documents=True)"
      ],
      "metadata": {
        "id": "JFnx-RauKDlr"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Does Apple sell my personal data?\"\n",
        "result = chain({\"query\": query})"
      ],
      "metadata": {
        "id": "V6zq6Y3BJkwx"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(result['result'].strip()))"
      ],
      "metadata": {
        "id": "uUh4FR_ZJslt",
        "outputId": "f26fcc4d-b5ee-4174-9375-18b8f8519a08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple does not sell your personal data to third parties for their own\n",
            "marketing purposes. However, Apple may share your personal data with\n",
            "Apple-affiliated companies, service providers, partners, developers,\n",
            "and publishers, or others at your direction. Apple may also share your\n",
            "personal data with third parties for legal obligations or business\n",
            "transfers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[source.metadata['organization'] for source in result['source_documents']]"
      ],
      "metadata": {
        "id": "GmPDfqMdPf0j",
        "outputId": "85ab448a-0dab-4bfd-c498-ed1ec9f0d354",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple', 'Apple', 'Apple', 'Threads']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-querying retriever.\n",
        "\n",
        "A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters."
      ],
      "metadata": {
        "id": "E6JIwM0QSnRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet lark"
      ],
      "metadata": {
        "id": "cmk2YL-RXTqB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "\n",
        "metadata_field_info=[\n",
        "    AttributeInfo(\n",
        "        name=\"organization\",\n",
        "        description=\"The organization\",\n",
        "        type=\"string or list[string]\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"title\",\n",
        "        description=\"The title of the document\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"url\",\n",
        "        description=\"The url for the document\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "]\n",
        "document_content_description = \"A policy\"\n",
        "retriever = SelfQueryRetriever.from_llm(llm, vectordb, document_content_description, metadata_field_info, verbose=True)"
      ],
      "metadata": {
        "id": "lsS_U-kdS1LY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\"Does Google use cookies?\")\n"
      ],
      "metadata": {
        "id": "rq_8AfDdaQBa",
        "outputId": "6a049d92-7128-4756-e976-57f4e0f0158c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-df7bf19fdcdd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_relevant_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Does Google use cookies?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_OmkbFstaWmG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}